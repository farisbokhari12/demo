# Weekly Incident Review — Week 50, 2024

## Summary

This week we had three incidents that impacted production. Below is a summary of each, root causes, and recommended alerting improvements.

---

## Incident 1: Payment Service Database Connection Pool Exhaustion

**Severity**: P1  
**Duration**: 45 minutes  
**Impact**: Customers unable to complete checkout

### What Happened

During a traffic spike on Tuesday afternoon, the payment service exhausted its database connection pool. New requests couldn't acquire connections and timed out, causing checkout failures for approximately 2,000 customers.

### Root Cause

Connection pool was sized for normal traffic (50 connections). The spike hit 3x normal volume and we had no visibility into pool utilization until it was fully exhausted.

### Alerts Needed

- Connection pool utilization alert at 75% and 90%
- Connection wait time alert when requests queue for connections
- Traffic anomaly detection for sudden volume spikes

---

## Incident 2: Order Service Memory Leak

**Severity**: P2  
**Duration**: 6 hours (overnight)  
**Impact**: Pod restarts, brief order processing delays

### What Happened

A memory leak introduced in last week's release caused order service pods to gradually consume memory until hitting OOM limits. Kubernetes restarted pods throughout the night, causing intermittent processing delays.

### Root Cause

A caching bug held references to completed orders instead of releasing them. Memory grew ~100MB/hour until hitting the 2GB limit.

### Alerts Needed

- Memory utilization trending alert (growth rate)
- Container restart count alert
- Memory approaching limit alert at 80%

---

## Incident 3: API Gateway Latency Spike

**Severity**: P2  
**Duration**: 30 minutes  
**Impact**: Slow response times during marketing campaign

### What Happened

A marketing email campaign drove a sudden traffic spike to the product catalog endpoints. The API gateway latency increased from 50ms to 2+ seconds, affecting all users—not just campaign traffic.

### Root Cause

Rate limiting wasn't configured per-endpoint. The traffic spike to catalog endpoints consumed gateway resources, impacting unrelated endpoints.

### Alerts Needed

- P95 latency alert when response time exceeds thresholds
- Request rate anomaly detection
- Per-endpoint latency monitoring

---

## Action Items

- [ ] Implement the 9 alerts identified above
- [ ] Review connection pool sizing for payment service
- [ ] Add memory profiling to order service CI pipeline
- [ ] Configure per-endpoint rate limiting on API gateway

---

## Attendees

- Platform Team Lead
- Payment Service Owner
- Order Service Owner
- SRE On-Call
